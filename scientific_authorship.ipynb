{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler, BatchSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords as stopwords_list\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "from nltk.tag.stanford import CoreNLPPOSTagger\n",
    "# from nltk.tag.stanford import StanfordTagger as CoreNLPPOSTagger\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether gpu is available\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleTextData():\n",
    "    authors = {}\n",
    "    texts = {}\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop = stopwords_list.words(\"english\")\n",
    "\n",
    "    def __init__(self, filedir=None, filedir_submitted=None, max_classes=None, min_class_freq=None, vocabulary_file=None, \n",
    "                 vocabulary=None, authors=None, texts = None, keys_mapping_file=None,\n",
    "                 stopwords_index=None, stopwords_dim=None,\n",
    "                references=None, texts_submitted=None,\n",
    "                classes=None, nr_labels=None, references_index=None, \n",
    "                 tail_authors=False, middle_authors=False,\n",
    "                references_dim=None, class_weights=None, just_abstract=False, \n",
    "                 just_contexts=False):\n",
    "        self.max_classes = max_classes\n",
    "        self.min_class_freq = min_class_freq\n",
    "        self.just_abstract = just_abstract\n",
    "        self.just_contexts = just_contexts\n",
    "        if authors:\n",
    "            self.vocabulary = vocabulary\n",
    "            self.vocabulary_size = len(self.vocabulary) + 1\n",
    "            self.authors = authors\n",
    "            self.texts = texts\n",
    "            self.stopwords_index = stopwords_index\n",
    "            self.stopwords_dim = stopwords_dim\n",
    "            self.references = references\n",
    "            self.references_index = references_index\n",
    "            self.references_dim = references_dim\n",
    "            self.classes = classes\n",
    "            self.nr_labels = nr_labels\n",
    "            self.class_weights = class_weights\n",
    "            self.texts_submitted = texts_submitted\n",
    "        else:\n",
    "            keys_mapping = None\n",
    "            if keys_mapping_file:\n",
    "                keys_mapping = {}\n",
    "                with open(keys_mapping_file) as f:\n",
    "                    for line in f:\n",
    "                        a1, a2 = line.strip().split(\",\")\n",
    "                        keys_mapping[a2] = a1\n",
    "            self._build_dataset(filedir, filedir_submitted, keys_mapping, vocabulary_file=vocabulary_file)\n",
    "            self.classes, self.nr_labels, self.class_weights = self._build_class_index(\n",
    "                max_classes=max_classes, min_class_freq=self.min_class_freq,\n",
    "                tail_authors=tail_authors, middle_authors=middle_authors)\n",
    "            self._build_references_index()\n",
    "        self.article_indices = [i for i in self.authors if i in self.texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.article_indices)\n",
    "    \n",
    "    @classmethod\n",
    "    def _collect_references(cls, filedir, keys_mapping=None):\n",
    "        filedir = filedir + \"/references/\"\n",
    "        filenames = os.listdir(filedir)\n",
    "        references = {}\n",
    "        for i, filename in enumerate(filenames):\n",
    "            article_index = filename.split(\".\")[0]\n",
    "            articles = {}\n",
    "\n",
    "            with open(os.path.join(filedir, filename)) as infile:\n",
    "                contents = infile.read()\n",
    "                soup = BeautifulSoup(contents,'xml')\n",
    "                article_nodes = soup.find_all('biblStruct')\n",
    "                for article_node in article_nodes:\n",
    "                    title = article_node.find_all(\"title\")[0].get_text()\n",
    "                    articles[title] = []\n",
    "                    authors_nodes = article_node.find_all(\"author\")\n",
    "                    for author_node in authors_nodes:\n",
    "                        names = []\n",
    "                        for name_node in author_node.find_all(\"forename\"):\n",
    "                            names.append(name_node.get_text().strip())\n",
    "                        for name_node in author_node.find_all(\"surname\"):\n",
    "                            names.append(name_node.get_text().strip())\n",
    "                        if names:\n",
    "                            articles[title].append(\" \".join(names))\n",
    "            if keys_mapping:\n",
    "                references[keys_mapping[article_index]] = articles\n",
    "            else:\n",
    "                references[article_index] = articles\n",
    "        return references\n",
    "            \n",
    "    @classmethod\n",
    "    def _normalize_authors(cls, authors):\n",
    "        normalized_authors = []\n",
    "        for author in authors:\n",
    "            author_names = author.split()\n",
    "            # take inital of first name and full last name\n",
    "            try:\n",
    "                normalized_name = \" \".join([author_names[0][0], author_names[-1]])\n",
    "            except:\n",
    "                sys.stderr.write(\"Could not normalize name %s\\n\" % author)\n",
    "            normalized_authors.append(normalized_name)\n",
    "        return normalized_authors\n",
    "\n",
    "    @classmethod\n",
    "    def _collect_authors(cls, filedir):\n",
    "        filedir = filedir + \"/headers/\"\n",
    "        filenames = os.listdir(filedir)\n",
    "        for i, filename in enumerate(filenames):\n",
    "            article_index = filename.split(\".\")[0]\n",
    "            cls.authors[article_index] = {}\n",
    "            with open(os.path.join(filedir, filename)) as infile:\n",
    "                contents = infile.read()\n",
    "                soup = BeautifulSoup(contents,'xml')\n",
    "                title = soup.find_all('title')[0].get_text()\n",
    "                cls.authors[article_index]['title'] = title\n",
    "                authors_nodes = soup.find_all('author')\n",
    "                cls.authors[article_index]['authors'] = []\n",
    "                for author_node in authors_nodes:\n",
    "                    names = []\n",
    "                    for name_node in author_node.find_all('forename'):\n",
    "                        name = name_node.get_text().strip()\n",
    "                        # the first (not middle) name is 1-letter long\n",
    "                        if len(name.split()[0]) == 1 and not names:\n",
    "                            sys.stderr.write(\"Suspicious name: %s, article %s\\n\" % (name, article_index))\n",
    "                        names.append(name)\n",
    "                    for name_node in author_node.find_all('surname'):\n",
    "                        name = name_node.get_text().strip()\n",
    "                        # exponents\n",
    "                        if name[-1] in '0123456789':\n",
    "                            name = name[:-1]\n",
    "                        names.append(name)\n",
    "                    if len(names) > 3:\n",
    "                        sys.stderr.write(\"Suspicious name: %s\\n\" % \" \".join(names))\n",
    "                    if names:\n",
    "                        # TODO: test of name containing non-letter characters\n",
    "                        cls.authors[article_index]['authors'].append(\" \".join(names))\n",
    "\n",
    "                \n",
    "    def _collect_texts(self, filedir, keys_mapping=None):\n",
    "        filenames = glob.glob(filedir + \"/contents/*.xml\")\n",
    "        excluded_tags = {'ref'}\n",
    "        context_window = 50\n",
    "        ref_contexts = {}\n",
    "        texts = {}\n",
    "        for i, filename in enumerate(filenames):\n",
    "            article_index = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            with open(filename) as infile:\n",
    "                contents = infile.read()\n",
    "                soup = BeautifulSoup(contents,'xml')\n",
    "                body = soup.find_all('text')[0].find_all('body')[0]\n",
    "                abstract = soup.find_all('abstract')[0]\n",
    "                title = soup.find_all('title')[0]\n",
    "                all_texts = []\n",
    "                ref_contexts_local = []\n",
    "                for refel in body.find_all('ref'): \n",
    "                    if refel.previous and type(refel.previous) is bs4.element.NavigableString: \n",
    "                        ref_contexts_local.append(refel.previous[-context_window:]) \n",
    "                    if refel.next_sibling and type(refel.next_sibling) is bs4.element.NavigableString:\n",
    "                        ref_contexts_local.append(refel.next_sibling[:context_window]) \n",
    "                    refel.replace_with(\" \")\n",
    "                elements = title.find_all() + abstract.find_all()\n",
    "                if not self.just_abstract:\n",
    "                    elements.extend(body.find_all())\n",
    "                for el in elements:\n",
    "                    if el.name not in excluded_tags:\n",
    "                        all_texts.append(el.get_text())\n",
    "                text = \" \".join(filter(None, all_texts))\n",
    "                # fix syllabified words\n",
    "                text = re.sub(\"- \", \"\", text) # TODO: check\n",
    "                if keys_mapping:\n",
    "                    texts[keys_mapping[article_index]] = text\n",
    "                    ref_contexts[article_index] = \" \".join(ref_contexts_local)\n",
    "                else:\n",
    "                    texts[article_index] = text\n",
    "                    ref_contexts[article_index] = \" \".join(ref_contexts_local)\n",
    "        if self.just_contexts:\n",
    "            return ref_contexts\n",
    "        else:\n",
    "            return texts\n",
    "    \n",
    "    def random_split(self, proportion1, proportion2=0, max_classes=None,\n",
    "            min_class_freq=None, tail_authors=False, middle_authors=False):\n",
    "        \n",
    "        # Split all text indices\n",
    "        indices1 = set()\n",
    "        indices2 = set()\n",
    "        indices3 = set()\n",
    "        articles_for_author = {}\n",
    "        \n",
    "        for article, article_metadata in self.authors.items():\n",
    "            author_list = article_metadata['authors']\n",
    "            for a in author_list:\n",
    "                if a not in self.classes:\n",
    "                    continue\n",
    "                if a not in articles_for_author:\n",
    "                    articles_for_author[a] = []\n",
    "                articles_for_author[a].append(article)\n",
    "\n",
    "        sorted_articles_per_author = sorted(articles_for_author.items(), key=lambda x: len(x[1]), reverse=False)\n",
    "        for author, articles in sorted_articles_per_author:\n",
    "            # take first article that is not already in second set, add it to first\n",
    "            for i in range(len(articles)):\n",
    "                if articles[i] not in indices2 and articles[i] not in indices3:\n",
    "                    indices1.add(articles[i])\n",
    "                    break\n",
    "            # take first article that is not already in first set, add it to second\n",
    "            for i in range(len(articles)):\n",
    "                if articles[i] not in indices1 and articles[i] not in indices3:\n",
    "                    indices2.add(articles[i])\n",
    "                    break\n",
    "            if proportion2: # we want to split it 3-ways\n",
    "                for i in range(len(articles)):\n",
    "                    if articles[i] not in indices1 and articles[i] not in indices2:\n",
    "                        indices3.add(articles[i])\n",
    "                        break\n",
    "            \n",
    "        # what is left\n",
    "        articles = set(self.texts.keys()).difference(indices2).difference(indices1).difference(indices3)\n",
    "        n1 = int(proportion1*len(self.texts.keys()))\n",
    "        if not proportion2:\n",
    "            p2 = 1-proportion1\n",
    "        else:\n",
    "            p2 = proportion2\n",
    "        n2 = int(proportion2*len(self.texts.keys()))\n",
    "        n1_remaining = max(n1-len(indices1), 0)\n",
    "        ia1 = set(random.sample(population=articles, k=min(n1_remaining, len(articles))))\n",
    "        indices1 = indices1.union(ia1)\n",
    "        n2_remaining = max(n2-len(indices2), 0)\n",
    "        articles = articles.difference(indices1)\n",
    "        ia2 = set(random.sample(population=articles, k=min(n2_remaining, len(articles))))\n",
    "        indices2 = indices2.union(ia2)\n",
    "        ia3 = articles.difference(indices1).difference(indices2)\n",
    "        indices3 = indices3.union(ia3)\n",
    "        \n",
    "        # Filter out authors that don't occur in all 3 index sets\n",
    "        valid_authors = set()\n",
    "        min_occs = 1000000\n",
    "        for aut, indices in articles_for_author.items():\n",
    "            # keep the author if it occurs in both sets\n",
    "            occs1, occs2, occs3 = len(set(indices).intersection(indices1)), len(set(indices).intersection(indices2)), len(set(indices).intersection(indices3))\n",
    "            if occs1 and occs2 and (occs3 or not indices3):\n",
    "                valid_authors.add(aut)\n",
    "                if occs1+occs2+occs3 < min_occs:\n",
    "                    min_occs = occs1+occs2+occs3\n",
    "\n",
    "        authors1 = {i: self.authors[i] for i in indices1}\n",
    "        authors2 = {i: self.authors[i] for i in indices2}\n",
    "        authors3 = {i: self.authors[i] for i in indices3}\n",
    "                    \n",
    "        # Filter out texts which don't have any of the valid authors\n",
    "        texts1 = {i: self.texts[i] for i in indices1 \n",
    "                  if i in self.texts and [a for a in authors1[i]['authors'] if a in valid_authors]} # TODO: why are there indices which are not in self.texts?        \n",
    "        texts2 = {i: self.texts[i] for i in indices2 \n",
    "                  if i in self.texts and [a for a in authors2[i]['authors'] if a in valid_authors]}\n",
    "        texts3 = {i: self.texts[i] for i in indices3 \n",
    "                  if i in self.texts and [a for a in authors3[i]['authors'] if a in valid_authors]}\n",
    "        texts_submitted = {i: self.texts_submitted[i] for i in texts3 if i in self.texts_submitted}\n",
    "        \n",
    "\n",
    "        print(\"minimum articles per author\", min_occs)\n",
    "\n",
    "        # Filter out authors which don't occur in any text anymore\n",
    "        authors_in1 = set()\n",
    "        authors_in2 = set()\n",
    "        authors_in3 = set()\n",
    "        for article, metadata in self.authors.items():\n",
    "            authors = metadata['authors']\n",
    "            if article not in texts1 and article not in texts2 and article not in texts3:\n",
    "                continue\n",
    "            if article in texts1:\n",
    "                authors_in1 = authors_in1.union(set([a for a in authors if a in valid_authors]))\n",
    "            if article in texts2:\n",
    "                authors_in2 = authors_in2.union(set([a for a in authors if a in valid_authors]))\n",
    "            if article in texts3:\n",
    "                authors_in3 = authors_in3.union(set([a for a in authors if a in valid_authors]))\n",
    "        valid_authors_with_texts = authors_in1.union(authors_in2).union(authors_in3)\n",
    "        # Rebuild the class index using the authors that are left  \n",
    "        classes, nr_labels, class_weights = self._build_class_index(\n",
    "            valid_authors = valid_authors_with_texts, max_classes=max_classes,\n",
    "            min_class_freq=min_class_freq, tail_authors=tail_authors, middle_authors=middle_authors)\n",
    "      \n",
    "        print(\"valid authors\", len(valid_authors), \"with texts\", len(valid_authors_with_texts))\n",
    "        data1 = ArticleTextData(authors=authors1, texts=texts1, vocabulary=self.vocabulary, \n",
    "                                max_classes=self.max_classes, stopwords_index=self.stopwords_index, \n",
    "                                stopwords_dim=self.stopwords_dim, references=self.references,\n",
    "                               classes=classes, nr_labels=nr_labels, references_index=self.references_index,\n",
    "                                references_dim=self.references_dim, class_weights=class_weights)\n",
    "        data2 = ArticleTextData(authors=authors2, texts=texts2, vocabulary=self.vocabulary, \n",
    "                                max_classes=self.max_classes, stopwords_index=self.stopwords_index,\n",
    "                               stopwords_dim=self.stopwords_dim, references=self.references,\n",
    "                               classes=classes, nr_labels=nr_labels, references_index=self.references_index,\n",
    "                                references_dim=self.references_dim, class_weights=class_weights)\n",
    "    \n",
    "        if proportion2:\n",
    "            data3 = ArticleTextData(authors=authors3, texts=texts3, vocabulary=self.vocabulary, \n",
    "                                max_classes=self.max_classes, stopwords_index=self.stopwords_index,\n",
    "                               stopwords_dim=self.stopwords_dim, references=self.references,\n",
    "                               classes=classes, nr_labels=nr_labels, references_index=self.references_index,\n",
    "                                references_dim=self.references_dim, class_weights=class_weights,\n",
    "                                   texts_submitted=texts_submitted)\n",
    "            \n",
    "            data4 = ArticleTextData(authors=authors3, texts=texts_submitted, vocabulary=self.vocabulary, \n",
    "                    max_classes=self.max_classes, stopwords_index=self.stopwords_index,\n",
    "                   stopwords_dim=self.stopwords_dim, references=self.references_submitted,\n",
    "                   classes=classes, nr_labels=nr_labels, references_index=self.references_index,\n",
    "                    references_dim=self.references_dim, class_weights=class_weights,\n",
    "                       texts_submitted=texts_submitted)\n",
    "            \n",
    "            return data1, data2, data3, data4\n",
    "        \n",
    "        return data1, data2\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "    def _build_vocabulary(self, min_freq=50, max_words=50000, vocabulary_file=None):\n",
    "        self.vocabulary = {}\n",
    "        self.stopwords_index = {self.stop[i]: i for i in range(len(self.stop))}\n",
    "        self.stopwords_dim = len(self.stopwords_index)\n",
    "        if vocabulary_file:\n",
    "            self.vocabulary = pickle.load(open(vocabulary_file, \"rb\"))\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "            return\n",
    "        word_freqs = Counter() \n",
    "        for art, text in self.texts.items():\n",
    "            tokenized_text = self._tokenize(text, filter_stopwords=True)\n",
    "            word_freqs.update(tokenized_text)\n",
    "        i = 1 # starts with 1, keeping 0 for padding\n",
    "        for word, freq in word_freqs.most_common():\n",
    "            if (min_freq and word_freqs[word] < min_freq) or i > max_words:\n",
    "                print(\"min freq is\", freq)\n",
    "                break\n",
    "            if re.match(\"^[0-9]+$\", word):\n",
    "                continue\n",
    "            self.vocabulary[word] = i\n",
    "            i += 1\n",
    "        self.vocabulary_size = len(self.vocabulary) + 1 # to include unknowns\n",
    " \n",
    "\n",
    "    @classmethod\n",
    "    def _tokenize(cls, text, filter_stopwords=False, keep_only_stopwords=False):\n",
    "        tokenized_text = cls.tokenizer.tokenize(text.lower())\n",
    "        if filter_stopwords:\n",
    "            filtered_text = list(filter(lambda w: w not in cls.stop, tokenized_text))\n",
    "        else:\n",
    "            filtered_text = tokenized_text\n",
    "        if keep_only_stopwords:\n",
    "            filtered_text = list(filter(lambda w: w in cls.stop, tokenized_text))\n",
    "        else:\n",
    "            filtered_text = list(filter(lambda w: len(w)>1, filtered_text))\n",
    "        return filtered_text\n",
    "\n",
    "    def _stopwords(cls, tokenized_text):\n",
    "        return [w for w in tokenized_text if w in cls.stop]\n",
    "    \n",
    "    def _build_references_index(self):\n",
    "        self.references_index = {}\n",
    "        cnt = 0\n",
    "        for article, references in self.references.items():\n",
    "            if article not in self.authors:\n",
    "                continue\n",
    "            article_authors = self.authors[article]['authors']\n",
    "            # only consider references in our dataset (covered by our limited classes)\n",
    "            labels = [self.classes.get(a, -1) for a in article_authors]\n",
    "            if all([label<0 for label in labels]):\n",
    "                continue\n",
    "            for title, authors in references.items():\n",
    "                authors = self._normalize_authors(authors)\n",
    "                for author in authors:\n",
    "                    if author not in self.references_index:\n",
    "                        self.references_index[author] = cnt\n",
    "                        cnt += 1\n",
    "        self.references_dim = len(self.references_index)\n",
    "    \n",
    "    @classmethod\n",
    "    def _build_class_index(cls, max_classes=None, min_class_freq=None, valid_authors=None, \n",
    "                           top_authors=False, tail_authors=False, middle_authors=False):\n",
    "        class_freq = Counter()\n",
    "        if not tail_authors and not middle_authors and max_classes:\n",
    "            top_authors=True\n",
    "        for article, article_metadata in cls.authors.items():\n",
    "            author_list = article_metadata['authors']\n",
    "            class_freq.update(author_list)\n",
    "        if top_authors:\n",
    "            all_authors = class_freq.most_common(max_classes)\n",
    "        if tail_authors:\n",
    "            all_authors = []\n",
    "            for a, f in sorted(class_freq.items(), key=lambda t:t[1], reverse=False):\n",
    "                if f >= min_class_freq:\n",
    "                    all_authors.append((a,f))\n",
    "                if len(all_authors) > max_classes:\n",
    "                    break\n",
    "        middle_authors_i1 = 0\n",
    "        middle_authors_i2 = len(class_freq)\n",
    "        if middle_authors:\n",
    "            all_authors = []\n",
    "            for a, f in class_freq.most_common():\n",
    "                if min_class_freq and (f < min_class_freq):\n",
    "                    continue\n",
    "                if valid_authors and a not in valid_authors:\n",
    "                    continue\n",
    "                all_authors.append((a,f))\n",
    "            total_authors = len(all_authors)\n",
    "            print(\"total authors\", total_authors)\n",
    "            middle_authors_i1 = max((total_authors - max_classes)/2, 0)\n",
    "            middle_authors_i2 = min(middle_authors_i1 + max_classes, total_authors-1)\n",
    "        classes = {}\n",
    "        i = 0\n",
    "        for cnt, (author, freq) in enumerate(all_authors):\n",
    "            if min_class_freq and (freq < min_class_freq):\n",
    "                continue\n",
    "            if valid_authors and author not in valid_authors:\n",
    "                continue\n",
    "            if middle_authors and \\\n",
    "                ((cnt < middle_authors_i1) or (cnt > middle_authors_i2)):\n",
    "                continue\n",
    "            classes[author] = i\n",
    "            i += 1\n",
    "        nr_labels = len(classes)\n",
    "        class_weights = np.zeros(len(classes))\n",
    "        for l, i in classes.items():\n",
    "            class_weights[i] = 1./class_freq[l]\n",
    "#             print(\"weight of \", i, self.weights[i], authors_freq[l])\n",
    "        return classes, nr_labels, class_weights\n",
    "    \n",
    "    \n",
    "    def _build_dataset(self, filedir=\"/home/ana/code/research/acl_authorship/extracted\", filedir_submitted=None,\n",
    "                       keys_mapping=None,\n",
    "                       min_word_freq=5, max_article_len=20000,\n",
    "                      vocabulary_file=None):\n",
    "        self._collect_authors(filedir)\n",
    "        self.texts = self._collect_texts(filedir)\n",
    "        self.references = self._collect_references(filedir)\n",
    "        if filedir_submitted:\n",
    "            self.texts_submitted = self._collect_texts(filedir_submitted, keys_mapping)\n",
    "            self.references_submitted = self._collect_references(filedir_submitted, keys_mapping)\n",
    "        else:\n",
    "            self.texts_submitted = {}\n",
    "            self.references_submitted = {}\n",
    "        self._build_vocabulary(min_freq=min_word_freq, vocabulary_file=vocabulary_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, article_data, multilabel, segment_size=1500,\n",
    "                 pad=True, pretrained_embeddings_file = None, embeddings_size = None, max_article_len=20000,\n",
    "                valid_labels=None, extract_POS=True, context_window=5, context_max_words=1000):\n",
    "\n",
    "        self.article_data = article_data\n",
    "        self.multilabel = multilabel\n",
    "        self.segment_size = segment_size\n",
    "        self.pad = pad\n",
    "        self.context_window = context_window\n",
    "        self.context_max_words = context_max_words\n",
    "        self.pretrained_embeddings = None\n",
    "        self.nr_labels = self.article_data.nr_labels\n",
    "        self.max_article_len = max_article_len\n",
    "        self.parser = None#CoreNLPPOSTagger()\n",
    "        self.extract_POS = extract_POS\n",
    "        self.POS_index = {pos: i+1 for i, pos in enumerate(['VBZ', 'VBG', 'PRP', 'VBN', 'IN', 'WRB', 'JJR', 'SYM', 'NNS', 'RB', 'TO', 'WDT', 'WP$', \n",
    "                 'RP', 'CD', 'NNPS', 'RBS', 'PRP$', 'NN', 'PDT', 'EX', 'FW', 'UH', 'WP', 'DT', 'VBP', \n",
    "                 'MD', 'CC', 'JJS', 'RBR', 'VB', 'JJ', 'LS', 'VBD', 'NNP'])}\n",
    "        if pretrained_embeddings_file:\n",
    "            self.pretrained_embeddings = pickle.load(open(pretrained_embeddings_file, \"rb\"))\n",
    "            self.embeddings_size = embeddings_size\n",
    "        self.valid_labels = valid_labels\n",
    "        self._encode_items()\n",
    "    \n",
    "    def _binarize_labels(self, n, labels):\n",
    "        binarized_labels = np.zeros(n)\n",
    "        for l in labels:\n",
    "            if l >= 0:\n",
    "                binarized_labels[l] = 1\n",
    "        return binarized_labels\n",
    "    \n",
    "    def _encode_items(self):\n",
    "        self.data = []\n",
    "        self.weights = []\n",
    "        self.source_articles = []\n",
    "        context_window = self.context_window\n",
    "        for article, text in self.article_data.texts.items():\n",
    "            author_list = self.article_data.authors[article]['authors']\n",
    "            labels = [self.article_data.classes.get(a, -1) for a in author_list]\n",
    "            if self.valid_labels:\n",
    "                labels = [l for l in labels if l in self.valid_labels]\n",
    "            if not labels or all([label<0 for label in labels]):\n",
    "                continue\n",
    "            tokenized_text = self.article_data._tokenize(text, filter_stopwords=False)\n",
    "            references = Counter()\n",
    "            try:\n",
    "                referenced_articles = self.article_data.references[article]\n",
    "                for title, authors in referenced_articles.items():\n",
    "                    authors = self.article_data._normalize_authors(authors)\n",
    "                    references.update(authors)\n",
    "            except KeyError:\n",
    "                sys.stderr.write(\"No references for article %s\\n\" % article)\n",
    "            if len(tokenized_text) <= 0 or len(tokenized_text) > self.max_article_len:\n",
    "                continue\n",
    "\n",
    "            for i in range(0, max(len(tokenized_text)-self.segment_size, len(tokenized_text)), self.segment_size):\n",
    "                segment = tokenized_text[i: i+self.segment_size]\n",
    "#                 print(segment)\n",
    "                if len(segment) < self.segment_size:\n",
    "                    if not self.pad:\n",
    "                        continue\n",
    "                if self.extract_POS:\n",
    "                    try:\n",
    "                        text_POS = [pos for (w, pos) in self.parser.tag(segment)]\n",
    "                    except Exception:\n",
    "#                         print(\"Could not extract POS for text \" + text)\n",
    "                        text_POS = []\n",
    "                else:\n",
    "                    text_POS = []\n",
    "                text_stopwords = Counter()\n",
    "                # Note: make sure you don't remove stopwords from segment/tokenized_text before doing this\n",
    "                for w in segment:\n",
    "                    if w in self.article_data.stopwords_index:\n",
    "                        text_stopwords[w] += 1\n",
    "                if self.multilabel:\n",
    "                    binarized_labels = self._binarize_labels(self.nr_labels, labels)\n",
    "                    self.data.append((segment, text_stopwords, text_POS, references, binarized_labels, article))\n",
    "                else:\n",
    "                    for label in labels:\n",
    "                        if label < 0:\n",
    "                            continue\n",
    "                        self.data.append((segment, text_stopwords, text_POS, references, label, article))\n",
    "                        self.weights.append(self.article_data.class_weights[label])\n",
    "                        self.source_articles.append(article)\n",
    "\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _random_embedding(self, n):\n",
    "        # normally distributed values between -1 and 1, mean 0\n",
    "        return np.random.randn(n)/3.\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        segment, stopwords, POS, references, label, article = self.data[i]\n",
    "        if not self.pretrained_embeddings:\n",
    "            encoded_text = [self.article_data.vocabulary.get(word, self.article_data.vocabulary_size-1) \n",
    "                                    for word in segment]\n",
    "\n",
    "            if len(segment) < self.segment_size and self.pad:\n",
    "                encoded_text = encoded_text + [0] * (self.segment_size - len(encoded_text))\n",
    "\n",
    "        encoded_stopwords = np.zeros(self.article_data.stopwords_dim)\n",
    "        # Note: sometimes the tagger merges a few tokens into one, so the sequence gets shorter, so it\n",
    "        # needs to be padded\n",
    "        # Sometimes it splits words in 2 (can/not) and needs to be trimmed\n",
    "        encoded_POS = np.zeros((max(self.POS_index.values())+1, self.segment_size))\n",
    "        for i, pos in enumerate(POS):\n",
    "            if i < encoded_POS.shape[1]:\n",
    "                encoded_POS[self.POS_index[pos]][i] = 1\n",
    "        for w, c in stopwords.items():\n",
    "            encoded_stopwords[self.article_data.stopwords_index[w]] = c\n",
    "        encoded_references = np.zeros(self.article_data.references_dim)\n",
    "        for r, c in references.items():\n",
    "            if r in self.article_data.references_index:\n",
    "                encoded_references[self.article_data.references_index[r]] = c\n",
    "        encoded_item = ((torch.tensor(encoded_text, dtype=torch.int64), \n",
    "                         torch.tensor(encoded_stopwords, dtype=torch.float32),\n",
    "                         torch.tensor(encoded_POS, dtype=torch.float32),\n",
    "                        torch.tensor(encoded_references, dtype=torch.float32)),\n",
    "                        torch.tensor(label, dtype=torch.int64),\n",
    "                       article)\n",
    "        return encoded_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "text_dataset = ArticleTextData(filedir=\"acl_pre2014/\", max_classes=None,min_class_freq=3,\n",
    "                              vocabulary='vocabulary.pkl')\n",
    "# Note: min_class_freq refers to number of articles not datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_articles, validation_articles, test_articles, test_articles_submitted = \\\n",
    "    text_dataset.random_split(0.8, 0.1, max_classes=200, middle_authors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_dataset = ArticleDataset(training_articles, multilabel=False, segment_size=sequence_length,\n",
    "                                 extract_POS=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove labels in validation set that don't occur in the training set\n",
    "\n",
    "training_labels = []\n",
    "for i, x in enumerate(training_dataset):\n",
    "    features, labels, article = x\n",
    "    print([f.shape for f in features], article)\n",
    "    training_labels.append(labels.item())\n",
    "    \n",
    "training_labels_set = set(training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_dataset = ArticleDataset(validation_articles, multilabel=True, segment_size=sequence_length,\n",
    "                                   valid_labels=training_labels_set, extract_POS=True) \n",
    "validation_dataset_single = ArticleDataset(validation_articles, multilabel=False, segment_size=sequence_length,\n",
    "                                          valid_labels=training_labels_set, extract_POS=True) \n",
    "\n",
    "validation_labels = []\n",
    "for i, x in enumerate(validation_dataset):\n",
    "    features, labels, article = x\n",
    "    labels_list = labels.nonzero().flatten().tolist()\n",
    "    print(labels_list)\n",
    "    validation_labels.extend(labels_list)\n",
    "    \n",
    "validation_labels2 = []\n",
    "for i, x in enumerate(validation_dataset_single):\n",
    "    features, labels, article = x\n",
    "    validation_labels2.append(labels.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove labels in training set that don't occur in the validation set\n",
    "\n",
    "training_dataset = ArticleDataset(training_articles, multilabel=False, segment_size=sequence_length,\n",
    "                                 extract_POS=True, valid_labels=set(validation_labels))\n",
    "training_labels = []\n",
    "for i, x in enumerate(training_dataset):\n",
    "    features, labels, article = x\n",
    "    training_labels.append(labels.item())\n",
    "    \n",
    "    \n",
    "training_labels_set = set(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ArticleDataset(test_articles, multilabel=True, segment_size=sequence_length,\n",
    "                                   valid_labels=None, extract_POS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for i, x in enumerate(test_dataset):\n",
    "    features, labels, article = x\n",
    "    labels_list = labels.nonzero().flatten().tolist()\n",
    "    print(labels_list)\n",
    "    test_labels.extend(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not set(validation_labels).difference(set(training_labels)))\n",
    "assert(training_dataset.article_data.references_index == validation_dataset.article_data.references_index)\n",
    "assert(training_dataset.article_data.classes == validation_dataset.article_data.classes)\n",
    "assert not set(list(training_articles.texts.keys())).intersection(set(list(validation_articles.texts.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not set(list(training_articles.texts.keys())).intersection(set(list(test_articles.texts.keys())))\n",
    "# assert(not set(test_labels).difference(set(training_labels)))\n",
    "assert(training_dataset.article_data.classes == test_dataset.article_data.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not set(list(training_dataset.article_data.texts.keys())).intersection(set(list(articles_from_files.texts.keys())))\n",
    "# assert(not set(test_labels).difference(set(training_labels)))\n",
    "assert(training_dataset.article_data.classes == test_datapoint.article_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = pickle.load(open(\"datasets_serialized/acl_vocabulary_vectors6.pkl\", \"rb\"))\n",
    "vocabulary = pickle.load(open(\"datasets_serialized/acl_vocabulary6.pkl\", \"rb\"))\n",
    "vectors = pickle.load(open(\"datasets_serialized/acl_vocabulary_vectors6.pkl\", \"rb\"))\n",
    "embedding_size = 300\n",
    "# random normal distribution with mean 0 from -1 to 1\n",
    "pretrained_embedding_weights = np.zeros((training_dataset.article_data.vocabulary_size, embedding_size))\n",
    "for w, i in training_dataset.article_data.vocabulary.items():\n",
    "    pretrained_embedding_weights[i] = pretrained_embeddings.get(w, np.random.randn(embedding_size)/3.)\n",
    "    \n",
    "print(pretrained_embedding_weights.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for features in training_dataset.data:\n",
    "    w, s, p, r, l, a = features\n",
    "    words.update(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_sampler = WeightedRandomSampler(training_dataset.weights, len(training_dataset))\n",
    "\n",
    "dataloader = DataLoader(training_dataset, batch_size=50,\n",
    "                        num_workers=4,\n",
    "                        sampler=training_sampler)\n",
    "all_labels_test = Counter()\n",
    "texts_lengths = []\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch)\n",
    "    features, labels, article = sample_batched\n",
    "    words, stopwords, pos, references = features\n",
    "    print(words.shape, stopwords.shape, references.shape, labels.shape)\n",
    "    labels_list = labels.tolist()\n",
    "    print(labels_list)\n",
    "    all_labels_test.update(labels_list)\n",
    "    if (i_batch>=2000):\n",
    "        break\n",
    "\n",
    "pd.Series(all_labels_test).plot(kind='hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_length=1500, filter_size=4, nr_filters=100, nr_classes=1, hidden_layers=[], \n",
    "                 vocab_size=20001, embedding_size=30, convolutions=True, pretrained_embeddings_size=300,\n",
    "                 stopwords_dim=179, references_dim=31889, references_hidden_dim=30, pos_dim=32,\n",
    "                dropout=0, nr_filters_pos=20, filter_size_pos=4,\n",
    "                 ignore_features=[False, False, False, False, False], pretrained_embedding_weights=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.seq_length_contexts = 1000\n",
    "        self.filter_size = filter_size\n",
    "        self.nr_filters = nr_filters\n",
    "        self.nr_classes = nr_classes\n",
    "        self.filter_size_pos = filter_size_pos\n",
    "        self.nr_filters_pos = nr_filters_pos\n",
    "        self.pos_dim = pos_dim\n",
    "        self.convolutions = convolutions\n",
    "        self.embeddings_size = embedding_size\n",
    "        self.dropout = dropout\n",
    "        self.ignore_features = ignore_features\n",
    "        if self.dropout:\n",
    "            self.dropout_layer1 = nn.Dropout(p=self.dropout)\n",
    "            self.dropout_layer2 = nn.Dropout(p=self.dropout)\n",
    "        prev_dim = 0\n",
    "\n",
    "        if not ignore_features[0]:\n",
    "            self.pretrained_embeddings_size = pretrained_embeddings_size\n",
    "            if self.pretrained_embeddings_size:\n",
    "                self.embeddings_size = self.pretrained_embeddings_size\n",
    "            else: \n",
    "                self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "                self.embedding.weights = pretrained_embedding_weights\n",
    "            prev_dim = self.embeddings_size\n",
    "            if convolutions:\n",
    "                self.conv1 = nn.Conv1d(in_channels=self.embeddings_size, out_channels=self.nr_filters, \n",
    "                                       kernel_size=self.filter_size)\n",
    "                self.conv_output_dim = self.seq_length - self.filter_size + 1\n",
    "\n",
    "                self.pool = nn.MaxPool1d(kernel_size=self.conv_output_dim)\n",
    "                prev_dim = self.nr_filters\n",
    "\n",
    "        \n",
    "        if not ignore_features[2]:\n",
    "            self.pos_conv = nn.Conv1d(in_channels=self.pos_dim, out_channels=self.nr_filters_pos, \n",
    "                                      kernel_size=self.filter_size_pos)\n",
    "            self.pos_conv_output_dim = self.seq_length - self.filter_size_pos + 1\n",
    "            self.pos_pool = nn.MaxPool1d(kernel_size=self.pos_conv_output_dim)\n",
    "            prev_dim += self.nr_filters_pos\n",
    "            \n",
    "        if not ignore_features[3]:\n",
    "            if references_hidden_dim:\n",
    "                self.references_hidden_layer = nn.Linear(references_dim, references_hidden_dim)\n",
    "            else:\n",
    "                self.references_hidden_layer = None\n",
    "                references_hidden_dim = references_dim\n",
    "            prev_dim +=  references_hidden_dim\n",
    "\n",
    "        self.hidden = nn.ModuleList([])\n",
    "        if not ignore_features[1]:\n",
    "            prev_dim += stopwords_dim\n",
    "        for hidden_layer in hidden_layers:\n",
    "            if not hidden_layer:\n",
    "                continue\n",
    "            self.hidden.append(nn.Linear(prev_dim, hidden_layer))\n",
    "            prev_dim = hidden_layer\n",
    "        # output layer\n",
    "        self.fc = nn.Linear(prev_dim, nr_classes, bias=True)\n",
    "    \n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        if not self.ignore_features[0]:\n",
    "            if not self.pretrained_embeddings_size:\n",
    "                features[0] = self.embedding(features[0])\n",
    "                if self.dropout:\n",
    "                    features[0] = self.dropout_layer1(features[0])\n",
    "\n",
    "            if self.convolutions:\n",
    "                features[0] = features[0].transpose(1,2)\n",
    "                features[0] = F.relu(self.conv1(features[0]))\n",
    "                features[0] = self.pool(features[0])\n",
    "                # flatten\n",
    "                features[0] = features[0].view(-1, self.num_flat_features(features[0]))\n",
    "\n",
    "            else:\n",
    "                features[0] = features[0].sum(dim=1)\n",
    "        if not self.ignore_features[2]:\n",
    "            features[2] = F.relu(self.pos_conv(features[2]))\n",
    "            features[2] = self.pos_pool(features[2])\n",
    "            features[2] = features[2].view(-1, self.num_flat_features(features[2]))\n",
    "\n",
    "        if not self.ignore_features[3]:\n",
    "            if self.references_hidden_layer:\n",
    "                features[3] = F.relu(self.references_hidden_layer(features[3]))\n",
    "\n",
    "        x = torch.cat([f for i, f in enumerate(features) if not self.ignore_features[i]], dim=1)\n",
    "        for i, hidden in enumerate(self.hidden):\n",
    "            x = F.relu(hidden(x))\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout_layer2(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_recall(predicted_ranking, true_authors):\n",
    "    '''Computes average precision and recall for a given article.'''\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    reciprocal_rank = 0\n",
    "    for rank, _ in enumerate(predicted_ranking):\n",
    "        predicted_authors = predicted_ranking[:rank+1]\n",
    "        correct_predictions = set(predicted_authors).intersection(set(true_authors))\n",
    "        precision = float(len(correct_predictions))/(rank+1)\n",
    "        recall = float(len(correct_predictions))/len(true_authors)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "\n",
    "        # find first rank where at least one prediction is correct\n",
    "        if len(correct_predictions) > 0:\n",
    "            if reciprocal_rank == 0:\n",
    "                reciprocal_rank = 1./(rank+1)\n",
    "    return sum(precisions)/len(precisions), sum(recalls)/len(recalls), reciprocal_rank, precisions, recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, data, topn=10, loss=None,  gpu=False, max_batches=20):\n",
    "    loader = DataLoader(data, batch_size=50,\n",
    "                        shuffle=True, num_workers=1)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    RRs = []\n",
    "    net.eval()\n",
    "    for i, batch in enumerate(loader):\n",
    "        features, labels, article = batch\n",
    "        print(features[0], features[0].shape)\n",
    "        if gpu:\n",
    "            outputs = net([f.cuda() for f in features])\n",
    "        else:\n",
    "            outputs = net(features)\n",
    "        if loss:\n",
    "            if gpu:\n",
    "                loss_size = loss(outputs, labels.cuda())\n",
    "            else:\n",
    "                loss_size = loss(outputs, labels)\n",
    "            losses.append(loss_size.item())\n",
    "        predictions = torch.sort(outputs, descending=True, dim=1)[1][:,:topn].cpu()\n",
    "        if not loss: # it means these are multi labels\n",
    "            labels_list = [l.nonzero().flatten() for l in labels]\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                precision, recall, RR, _, _ = average_precision_recall(predictions[i].tolist(), labels_list[i].tolist())\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                RRs.append(RR)\n",
    "        else:\n",
    "            labels_list = labels.tolist()\n",
    "        correct = [len(np.intersect1d(predictions[j], labels_list[j]))>0 for j in range(len(predictions))]\n",
    "        total_correct += sum(correct)\n",
    "        total += len(correct)\n",
    "        if i > max_batches:\n",
    "            break\n",
    "    MAP = float(sum(precisions))/len(precisions) if precisions else None\n",
    "    MAR = float(sum(recalls))/len(recalls) if recalls else None\n",
    "    MRR = float(sum(RRs))/len(RRs) if RRs else None\n",
    "    accuracy = float(total_correct)/total\n",
    "    return {'accuracy': accuracy, 'loss': losses, 'MAP': MAP, 'MAR': MAR, 'MRR': MRR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"epochs\": 150,\n",
    "              \"learning_rate\": 0.0001,\n",
    "              \"batch_size\": 128,\n",
    "              'hidden_layers': 0,\n",
    "              'weight_decay': 0.0001,\n",
    "              'convolutions': True,\n",
    "               'filters': 300,\n",
    "               'filter_width': 9,\n",
    "               'nr_filters_pos': 50,\n",
    "               'references_hidden_dim': 1000,\n",
    "              'lr_decrease_step': 5,\n",
    "              'lr_decrease_rate': 10,\n",
    "               'dropout': 0.5,\n",
    "              'optimizer': 'adam',\n",
    "               'momentum': 0.9,\n",
    "               'just_abstract': False,\n",
    "               'just_context': False,\n",
    "              'ignore_features': [False, False, False, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "training_accuracies = []\n",
    "validation_losses = []\n",
    "max_batches = 1000000\n",
    "all_predictions = Counter()\n",
    "all_labels = Counter()\n",
    "eval_freq = 200\n",
    "\n",
    "def train(hyperparams, training_data, validation_data, validation_data_single, training_validation_data=None, \n",
    "          gpu=False, log_experiment=False, experiment_tags=[], cometml_api_key=\"\"):\n",
    "    net = Net(filter_size=hyperparams['filter_width'], nr_filters=hyperparams['filters'], hidden_layers=[hyperparams['hidden_layers']], \n",
    "              nr_classes=training_dataset.nr_labels, nr_filters_pos=hyperparams['nr_filters_pos'],\n",
    "              vocab_size=training_dataset.article_data.vocabulary_size, convolutions=hyperparams['convolutions'],\n",
    "              seq_length=sequence_length, stopwords_dim=training_data.article_data.stopwords_dim,\n",
    "              references_dim=training_data.article_data.references_dim,\n",
    "              references_hidden_dim=hyperparams['references_hidden_dim'], pos_dim=max(training_dataset.POS_index.values())+1,\n",
    "             embedding_size=300, pretrained_embeddings_size=0, dropout=hyperparams['dropout'],\n",
    "             ignore_features=hyperparams['ignore_features'])\n",
    "    experiment = Experiment(api_key=cometml_api_key,\n",
    "                        project_name=\"general\", workspace=\"\", disabled=not log_experiment)\n",
    "    print(net)\n",
    "            \n",
    "    experiment.add_tags(experiment_tags + [\"correct\", \"correct_pos\"])\n",
    "    experiment.log_parameters(hyperparams)\n",
    "    experiment.log_parameters({\"seq_length\": sequence_length, \n",
    "                               \"vocab_size\": training_dataset.article_data.vocabulary_size,\n",
    "                              \"nr_classes\": training_data.nr_labels})\n",
    "    if gpu:\n",
    "        net = net.cuda()\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    #Optimizer\n",
    "    optimizer_names = {\n",
    "        'adam': torch.optim.Adam,\n",
    "        'adagrad': torch.optim.Adagrad,\n",
    "        'sgd': torch.optim.SGD,\n",
    "        'adadelta': torch.optim.Adadelta\n",
    "    }\n",
    "    optimizer_class = optimizer_names[hyperparams['optimizer']]\n",
    "    optimizer = optimizer_class(net.parameters(), lr=hyperparams['learning_rate'], \n",
    "                                weight_decay=hyperparams['weight_decay'])\n",
    "    max_acc = 0\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "    for e in range(hyperparams['epochs']):\n",
    "        train_loader = DataLoader(training_data, batch_size=hyperparams['batch_size'],\n",
    "                        sampler=training_sampler, num_workers=4)\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, labels, article = batch\n",
    "\n",
    "            if gpu:\n",
    "                outputs = net([f.cuda() for f in features])\n",
    "                loss_size = loss(outputs, labels.cuda())\n",
    "            else:\n",
    "                outputs = net(features)\n",
    "                loss_size = loss(outputs, labels)\n",
    "            print(\"Loss:\", loss_size.data.item())\n",
    "            losses.append(loss_size.data.item())\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predictions = outputs.cpu().sort(descending=True)[1][:,:10]\n",
    "            labels_list = labels.tolist()\n",
    "\n",
    "            correct = [len(np.intersect1d(predictions[j], labels_list[j])) for j in range(len(predictions))]\n",
    "            training_accuracy = sum(correct)/float(hyperparams['batch_size'])\n",
    "            training_accuracies.append(training_accuracy)\n",
    "\n",
    "            all_predictions.update(sum([p.tolist()[:10] for p in predictions], []))\n",
    "            all_labels.update(labels_list)\n",
    "\n",
    "            experiment.log_metric(\"train_acc\", float(training_accuracy))\n",
    "            experiment.log_metric(\"loss\", float(loss_size.item()))\n",
    "            \n",
    "            if i%eval_freq==0:\n",
    "                metrics = evaluate(net, validation_data, topn=10, gpu=gpu)\n",
    "                metrics2 = evaluate(net, validation_data_single, loss=loss, topn=10, gpu=gpu)\n",
    "\n",
    "                validation_losses.extend(metrics2['loss'])\n",
    "                print(metrics)\n",
    "                print(metrics2)\n",
    "                experiment.log_metric(\"acc\", float(metrics['accuracy']))\n",
    "                avg_eval_loss = float(sum(metrics2['loss']))/len(metrics2['loss'])\n",
    "                experiment.log_metric(\"eval_loss\", avg_eval_loss)\n",
    "                experiment.log_metric(\"MAP\", float(metrics['MAP']))\n",
    "                experiment.log_metric(\"MAR\", float(metrics['MAR']))\n",
    "                experiment.log_metric(\"MRR\", float(metrics['MRR']))\n",
    "                \n",
    "                if float(metrics['accuracy']) > max_acc:\n",
    "                    max_acc = float(metrics['accuracy'])\n",
    "                    with open(\"best_modelx.pkl\", \"wb+\") as f:\n",
    "                        pickle.dump(net, f)\n",
    "                \n",
    "                scheduler.step()\n",
    "                net.train()\n",
    "            accuracies.append(metrics['accuracy'])\n",
    "\n",
    "            \n",
    "            if i > max_batches:\n",
    "                break\n",
    "            print(\"batch\", i, \"epoch\", e, \"learning rate\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if log_experiment:\n",
    "        return experiment, net\n",
    "    else:\n",
    "        return net\n",
    "\n",
    "net=train(hyperparams, training_dataset, validation_dataset, validation_dataset_single, gpu=True, \n",
    "      log_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_article(net, data, topn=10, loss=None,  gpu=False, max_batches=1000, cutoff=10):\n",
    "    loader = DataLoader(data, batch_size=50,\n",
    "                        shuffle=False, num_workers=0)\n",
    "    outputs_for_article = {}\n",
    "    labels_for_article = {}\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    RRs = []\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        features, labels, articles = batch\n",
    "\n",
    "        if gpu:\n",
    "            outputs = net([f.cuda() for f in features])\n",
    "        else:\n",
    "            outputs = net(features)\n",
    "        if loss:\n",
    "            if gpu:\n",
    "                loss_size = loss(outputs, labels.cuda())\n",
    "            else:\n",
    "                loss_size = loss(outputs, labels)\n",
    "        for i, article in enumerate(articles):\n",
    "            output = outputs[i]\n",
    "            if article not in outputs_for_article:\n",
    "                outputs_for_article[article] = output\n",
    "            else:\n",
    "                outputs_for_article[article] = torch.cat((outputs_for_article[article], output))\n",
    "            labels_for_article[article] = [l[0] for l in labels[i].nonzero().tolist()]\n",
    "            \n",
    "  \n",
    "    for article in outputs_for_article:\n",
    "        outputs = outputs_for_article[article]\n",
    "        labels = labels_for_article[article]\n",
    "        probabilities, predictions = outputs.cpu().sort(descending=True)\n",
    "        # readjust indices to correspond to classes. the vector got longer than the number\n",
    "        # of classes when you concatenated everything\n",
    "        predictions = [p.item()%data.article_data.nr_labels for p in predictions]\n",
    "        top_predictions = []\n",
    "        for p in predictions:\n",
    "            if p not in top_predictions:\n",
    "                top_predictions.append(p)\n",
    "            if len(top_predictions) >= topn:\n",
    "                break\n",
    "\n",
    "                precision, recall, RR, _, _ = average_precision_recall(top_predictions, labels)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        RRs.append(RR)\n",
    "\n",
    "        correct = np.intersect1d(top_predictions, labels)\n",
    "        if len(correct)>0:\n",
    "            total_correct += 1\n",
    "        total += 1\n",
    "\n",
    "        MAP = float(sum(precisions))/len(precisions) if precisions else None\n",
    "    MAR = float(sum(recalls))/len(recalls) if recalls else None\n",
    "    MRR = float(sum(RRs))/len(RRs) if RRs else None\n",
    "    accuracy = float(total_correct)/total\n",
    "    return {'accuracy': accuracy, 'loss': losses, 'MAP': MAP, 'MAR': MAR, 'MRR': MRR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net = pickle.load(open(\"datasets_serialized/model_best_922.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test on the test set\n",
    "print(evaluate_per_article(trained_net.cuda(), test_dataset, gpu=True, max_batches=100000))\n",
    "print(evaluate(trained_net.cuda(), test_dataset, gpu=True, max_batches=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Optimizer\n",
    "optimizer = Optimizer(\"\") # key here\n",
    "# Declare your hyper-parameters:\n",
    "params = \"\"\"\n",
    "learning_rate real [0.00001, 0.01] [0.0001]\n",
    "batch_size integer [100, 512] [256]\n",
    "hidden_layers categorical {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000} [500]\n",
    "weight_decay real [0.001, 0.1] [0.01]\n",
    "lr_decrease_step integer [1, 20] [5]\n",
    "lr_decrease_rate integer [2, 10] [10]\n",
    "filters categorical {50, 70, 100, 150, 175, 200} [200]\n",
    "references_hidden_dim categorical {10, 50, 100, 200, 300, 500, 700, 1000, 1200, 1500, 1700, 2000} [500]\n",
    "dropout real [0.0, 0.5] [0.4]\n",
    "optimizer categorical {adam, adagrad, sgd} [adam]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "ignore_features categorical {[False, False, False, False], [False, True, True, True], [True, False, True, True], [True, True, False, True], [True, True, True, False]} [[False, False, False, False]]\n",
    "\"\"\"\n",
    "optimizer.set_params(params)\n",
    "\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "all_training_accuracies = []\n",
    "all_validation_losses = []\n",
    "all_hyperparams = []\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    training_accuracies = []\n",
    "    validation_losses = []\n",
    "    # Get a suggestion\n",
    "    suggestion = optimizer.get_suggestion()\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    hyperparams[\"learning_rate\"] = suggestion[\"learning_rate\"]\n",
    "    hyperparams[\"batch_size\"] = suggestion[\"batch_size\"]\n",
    "    hyperparams[\"hidden_layers\"] = suggestion[\"hidden_layers\"]\n",
    "    hyperparams[\"weight_decay\"] = suggestion[\"weight_decay\"]\n",
    "    hyperparams[\"lr_decrease_step\"] = suggestion[\"lr_decrease_step\"]\n",
    "    hyperparams[\"lr_decrease_rate\"] = suggestion[\"lr_decrease_rate\"]  \n",
    "    hyperparams[\"optimizer\"] = suggestion[\"optimizer\"]\n",
    "    hyperparams[\"references_hidden_dim\"] = suggestion[\"references_hidden_dim\"]\n",
    "    hyperparams[\"dropout\"] = suggestion[\"dropout\"]\n",
    "    experiment, net = train(hyperparams, training_dataset, validation_dataset, validation_dataset_single, \n",
    "                       gpu=True, experiment_tags=[\"tune\", \"correct_conv\", \"ablation\"])\n",
    "    score = sum(accuracies[-100:])/len(accuracies[-100:])\n",
    "    # Report the score back\n",
    "    suggestion.report_score(\"avg_accuracy\",score)\n",
    "    \n",
    "    all_losses.append(losses)\n",
    "    all_accuracies.append(accuracies)\n",
    "    all_validation_losses.append(validation_losses)\n",
    "    all_training_accuracies.append(training_accuracies)\n",
    "    all_hyperparams.append(hyperparams)\n",
    "    with open(\"experiments/experiment\" + experiment.id, \"wb+\") as log: \n",
    "        pickle.dump({'losses': losses, 'accuracies': accuracies, \n",
    "                     'training_accuracies': training_accuracies, 'hyperparams': hyperparams}, log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_list(dataset):\n",
    "    # TODO: issue: this doesn't remove occurrences of authors from articles\n",
    "    # where all the co-authors were removed so the article is no longer in\n",
    "    # the dataset?\n",
    "    # Actually not a problem cause we keep articles authored by any number of\n",
    "    # authors as long as there is at least one.\n",
    "    return Counter(sum([[a for a in p['authors'] if a in dataset.article_data.classes]\n",
    "                        for p in dataset.article_data.authors.values()], \n",
    "                  []))\n",
    "\n",
    "def get_references(dataset):\n",
    "    # TODO: issue: this doesn't remove occurrences of authors from articles\n",
    "    # where all the co-authors were removed so the article is no longer in\n",
    "    # the dataset?\n",
    "    # Actually not a problem cause we keep articles authored by any number of\n",
    "    # authors as long as there is at least one.\n",
    "    references_inexp = {p: v for (p, v) in dataset.article_data.references.items()\n",
    "                        if p in dataset.article_data.article_indices}\n",
    "    return Counter(ArticleTextData._normalize_authors([a for a in sum(sum([(list(p.values())) \n",
    "                            for p in references_inexp.values()], []), \n",
    "                       []) if a in dataset.article_data.classes]))\n",
    "\n",
    "\n",
    "training_authors_freq = get_authors_list(training_dataset)\n",
    "test_authors_freq = get_authors_list(test_dataset)\n",
    "validation_authors_freq = get_authors_list(validation_dataset)\n",
    "\n",
    "training_authors_freq.update(test_authors_freq)\n",
    "training_authors_freq.update(validation_authors_freq)\n",
    "training_authors_freq.most_common()\n",
    "\n",
    "training_references = get_references(training_dataset)\n",
    "test_references = get_references(test_dataset)\n",
    "validation_references = get_references(validation_dataset)\n",
    "\n",
    "training_references.update(test_references)\n",
    "training_references.update(validation_references)\n",
    "training_references.most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors_list = list(training_authors_freq.keys())\n",
    "print(len(all_authors_list))\n",
    "\n",
    "# Removing middle names\n",
    "all_authors_seminormalized = [\" \".join([a.split(\" \")[0], a.split(\" \")[-1]]) \n",
    "                              for a in all_authors_list]\n",
    "all_authors_normalized = ArticleTextData._normalize_authors(all_authors_seminormalized)\n",
    "print(len(all_authors_normalized))\n",
    "\n",
    "collisions_counter = Counter(all_authors_normalized)\n",
    "collisions_counter.most_common()\n",
    "# [a for a,f in collisions_counter.items() if f > 1]\n",
    "collisions_emnlp = [a for a in all_authors_list\n",
    "              if collisions_counter[ArticleTextData._normalize_authors([a])[0]] > 1]\n",
    "collisions_emnlp = sorted(collisions_emnlp, key=lambda n: n.split()[-1])\n",
    "# open(\"collisions_emnlp\", \"w+\").write(\"\\n\".join(collisions_emnlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"collisions_emnlp\", \"r\") as f:\n",
    "    collisions_emnlp_full = f.read().split(\"\\n\")\n",
    "[a for a in collisions_emnlp_full if a in training_dataset.article_data.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_freq_acl = training_authors_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_freq_emnlp = training_authors_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(8,4)\n",
    "ax[0].bar(range(len(authors_freq_acl)), \n",
    "        sorted(authors_freq_acl.values(), reverse=True), \n",
    "        width=2.75, log=False)\n",
    "ax[0].set_xlabel('ACL authors')\n",
    "ax[0].set_ylabel('Articles')\n",
    "\n",
    "ax[1].bar(range(len(authors_freq_emnlp)), \n",
    "        sorted(authors_freq_emnlp.values(), reverse=True), \n",
    "        width=2.75, log=False)\n",
    "ax[1].set_xlabel('EMNLP authors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"acl_author_freqs.csv\", \"w+\") as fa:\n",
    "    for a, f in authors_freq_acl.most_common():\n",
    "        fa.write(a + \",\" + str(f) + \"\\n\")\n",
    "\n",
    "with open(\"emnlp_author_freqs.csv\", \"w+\") as fa:\n",
    "    for a, f in authors_freq_emnlp.most_common():\n",
    "        fa.write(a + \",\" + str(f) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
